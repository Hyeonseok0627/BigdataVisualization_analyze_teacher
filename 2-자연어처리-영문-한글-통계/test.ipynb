{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영문 분석 -> 워드 클라우드로 그리기(시각화)\n",
    "\n",
    "## 샘플 데이터 영문 학술 문서의 제목만 추출, 그 단어의 빈도 분석 시각화 \n",
    "## 데이터 수집 : Big data 키워드로 검색 후, 해당 학술 연구 정보 서비스에서 수집 해보기. \n",
    "## 조합, pandas.concat(), 정제 re 정규식, 기본적인 유효성 체크. \n",
    "## 변환 : word_tokenize(), lower(), \n",
    "## matplotlib.pyplot 이용하기. \n",
    "## 단어 빈도 구해주는 Counter() 이용. \n",
    "\n",
    "## 비정형 빅데이터 분석을 말하고 -> 자연어 처리 (nature language processing )\n",
    "## 자연어 처리 예) 음성, 텍스트 정보 추출. \n",
    "\n",
    "## 단어 빈도를 추출해서, 해당 단어 시각화하기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 관련 단어 용어 정리 \n",
    "## 텍스트 분석: 자연어 처리와 데이터 마이닝 결합하여 발전되었고, \n",
    "## 비정형 텍스트 데이터에서 정보를 추출하는 분석 방법. \n",
    "## 분석 방법 : 1) 텍스트 분류 2) 텍스트 군집화 3) 감성 분석. \n",
    "\n",
    "## 전처리 : 분석 작업의 정확도를 높이기 위해서 사용할 데이터 정리하고 변환하는 작업. \n",
    "### 수행하는 작업 \n",
    "### 정제 (cleaning): 불필요한 기호, 문자 필터하는 작업, 정규식을 이용해서 작업을 함. \n",
    "### 정규화 ( normallization) : 형태가 다른 단어를 특정의 형태로 변환 작업 , 대문자, 소문자 통합 하는 작업, 의미가 비슷한 단어끼리 통합작업. \n",
    "### 토큰화 (tokenization) : 토큰으로 정하는 기본 단위로 분리 작업. 문장 기준, 단어 기준이 될수 도 있다. \n",
    "### 불용어제거(stopword) : 의미 있는 단어를 추출하기 위해서, 조사, 관사, 접미사, 접두사 등. 제거하는 작업. \n",
    "### 어간 추출(semming) : 단수, 복수, 진행형(시제), 분리하는 작업 \n",
    "### 표제어 추출(lemmatization ): 단어의 기본형 형태로 일반화 하는 작업. \n",
    "### 예) \n",
    "### Gone -> go \n",
    "### am -> be\n",
    "### going -> go \n",
    "\n",
    "## 워드클라우드 : 텍스트 분석에서 빈도를 시각화 할 때 많이 사용됨. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 수집 \n",
    "# 한국교육학술정보원 (KERIS)의 RISS 사이트 \n",
    "# https://www.riss.kr/index.do\n",
    "# Big data 검색해보기.\n",
    "# 한 페이지당 100개씩 내보내기 엑셀 파일 간략 정보 , 반복 10번 \n",
    "# 1000개의 데이터에서 제목만 추출 및 분류 작업하기. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 준비 작업. \n",
    "# 제목 컬럼 빈도 분석 해보기. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\python\\python310\\lib\\site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\python\\python310\\lib\\site-packages (from wordcloud) (1.26.2)\n",
      "Requirement already satisfied: pillow in c:\\python\\python310\\lib\\site-packages (from wordcloud) (10.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\python\\python310\\lib\\site-packages (from wordcloud) (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (4.46.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\it\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib->wordcloud) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python\\python310\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python\\python310\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\python\\python310\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\python\\python310\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python\\python310\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\python\\python310\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\it\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 관련 패키지들 임포트 하기. \n",
    "import pandas as pd \n",
    "# 경로 이름 지정해서 파일 처리할 때 사용하는 도구\n",
    "import glob \n",
    "# 정규 표현식에 사용하는 도구 \n",
    "import re \n",
    "# 2차원 리스트를 -> 1차원 리스트로 차원 축소시 사용하는 도구 \n",
    "from functools import reduce\n",
    "# 자연어 처리 패키지 중에서, 단어 토큰화 작업.\n",
    "from nltk.tokenize import word_tokenize\n",
    "# 불용어 처리 작업. \n",
    "from nltk.corpus import stopwords \n",
    "# 표제어 추출 \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "# 단어의 빈도를 추출하는 도구. \n",
    "from collections import Counter \n",
    "import matplotlib.pyplot as plt\n",
    "# 단어의 빈도수를 시각화하는 도구, 빈도가 높을수록 글자 크기가 커짐. \n",
    "from wordcloud import STOPWORDS, WordCloud \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>번호</th>\n",
       "      <th>제목</th>\n",
       "      <th>저자</th>\n",
       "      <th>출판사</th>\n",
       "      <th>출판일</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>A Big Data Guide to Understanding Climate Chan...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Mary Ann Leibert</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Deep Learning on Big, Sparse, Behavioral Data</td>\n",
       "      <td>De Cnudde, Sofie; Ramon, Yanou; Martens, David...</td>\n",
       "      <td>Mary Ann Leibert</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Call for Papers: Special Issue on Computationa...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Mary Ann Leibert</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>A new big data approach based on geoecological...</td>\n",
       "      <td>Varotsos, Costas A.; Krapivin, Vladimir F.</td>\n",
       "      <td>John Wiley &amp; Sons Ltd</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>Combining Human Computing and Machine Learning...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Mary Ann Leibert</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>HDM: A Composable Framework for Big Data Proce...</td>\n",
       "      <td>Wu, D.; Zhu, L.; Lu, Q.; Sakr, S.</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NaN</td>\n",
       "      <td>97</td>\n",
       "      <td>A Multi-Branch Decoder Network Approach to Ada...</td>\n",
       "      <td>Zhang, Yang</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "      <td>98</td>\n",
       "      <td>AlgorithmSeer: A System for Extracting and Sea...</td>\n",
       "      <td>Tuarob, S.; Bhatia, S.; Mitra, P.; Giles, C. L.</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NaN</td>\n",
       "      <td>99</td>\n",
       "      <td>Hybridisation of classifiers for anomaly detec...</td>\n",
       "      <td>Alguliyev, Rasim M.; Aliguliyev, Ramiz M.; Abd...</td>\n",
       "      <td>Inderscience</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>An Enhanced Visualization Method to Aid Behavi...</td>\n",
       "      <td>Fang, H.; Zhang, Z.</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0   번호                                                 제목  \\\n",
       "0          NaN    1  A Big Data Guide to Understanding Climate Chan...   \n",
       "1          NaN    2      Deep Learning on Big, Sparse, Behavioral Data   \n",
       "2          NaN    3  Call for Papers: Special Issue on Computationa...   \n",
       "3          NaN    4  A new big data approach based on geoecological...   \n",
       "4          NaN    5  Combining Human Computing and Machine Learning...   \n",
       "..         ...  ...                                                ...   \n",
       "95         NaN   96  HDM: A Composable Framework for Big Data Proce...   \n",
       "96         NaN   97  A Multi-Branch Decoder Network Approach to Ada...   \n",
       "97         NaN   98  AlgorithmSeer: A System for Extracting and Sea...   \n",
       "98         NaN   99  Hybridisation of classifiers for anomaly detec...   \n",
       "99         NaN  100  An Enhanced Visualization Method to Aid Behavi...   \n",
       "\n",
       "                                                   저자                    출판사  \\\n",
       "0                                             unknown       Mary Ann Leibert   \n",
       "1   De Cnudde, Sofie; Ramon, Yanou; Martens, David...       Mary Ann Leibert   \n",
       "2                                             unknown       Mary Ann Leibert   \n",
       "3          Varotsos, Costas A.; Krapivin, Vladimir F.  John Wiley & Sons Ltd   \n",
       "4                                             unknown       Mary Ann Leibert   \n",
       "..                                                ...                    ...   \n",
       "95                  Wu, D.; Zhu, L.; Lu, Q.; Sakr, S.                unknown   \n",
       "96                                        Zhang, Yang                unknown   \n",
       "97    Tuarob, S.; Bhatia, S.; Mitra, P.; Giles, C. L.                unknown   \n",
       "98  Alguliyev, Rasim M.; Aliguliyev, Ramiz M.; Abd...           Inderscience   \n",
       "99                                Fang, H.; Zhang, Z.                unknown   \n",
       "\n",
       "     출판일  \n",
       "0   2014  \n",
       "1   2019  \n",
       "2   2017  \n",
       "3   2017  \n",
       "4   2016  \n",
       "..   ...  \n",
       "95  2018  \n",
       "96  2022  \n",
       "97  2016  \n",
       "98  2019  \n",
       "99  2018  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 조합(병합)하기. \n",
    "# 현재 폴더 내부에 있는 , 받았던 엑셀 파일명 10개를 선택하기. \n",
    "all_files = glob.glob(\"./myCabinetExcelData*.xls\")\n",
    "all_files\n",
    "\n",
    "# 엑셀 파일 읽어서 -> 데이터 프레임 (표형태) 변환 ->특정 리스트에 담아두기 \n",
    "# 임시로 저장할 리스트 변수 \n",
    "all_files_data = []\n",
    "\n",
    "# all_files 에 담겨진 엑셀 파일의 위치가 들어있고, \n",
    "# 해당 위치의 엑셀 파일을 읽어서, 데이터 프레임 표 형태로 변환하기. \n",
    "# 임시 리스트에 담기. \n",
    "for file in all_files:\n",
    "  # 해당 엑셀 파일의 위치의 물리 파일 읽기\n",
    "  data_frame = pd.read_excel(file)\n",
    "  # 임시 리스트에 담기. \n",
    "  all_files_data.append(data_frame)\n",
    "\n",
    "# 샘플 확인 해보기., 첫번째 요소 확인 해보기. \n",
    "# all_files_data = [엑셀1,엑셀2,엑셀3,...엑셀10]\n",
    "all_files_data[0]\n",
    "\n",
    "# 오류 발생, 모듈 미설 : xlrd\n",
    "# cmd -> pip install xlrd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀 파일 10개를 병합해서 출력해보기. \n",
    "# axis=0 , 세로 방향으로 , 밑으로 데이터를 붙이는 작업. \n",
    "all_files_data_concat = pd.concat(all_files_data, axis=0, ignore_index=True)\n",
    "all_files_data_concat.shape\n",
    "\n",
    "# 병합된 파일을 csv 파일로 변환하기. \n",
    "all_files_data_concat.to_csv(\"./riss_Bigdata.csv\", encoding=\"utf-8\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
